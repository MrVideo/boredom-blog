---
title: 'Data centre buildings and infrastructures'
date: 2024-06-06T11:00:00+02:00
draft: false
type: 'page'
toc: true
mathjax: true
---

---

## Download as PDF

You can download this note as a PDF by clicking [here](dc-infrastructures.pdf).

---

A warehouse-scale computer has a lot of important components related to power delivery, cooling and building infrastructure that need to be considered.

## Power systems

In order to protect against power failure, battery and diesel generators are used to backup the external supply.

An Uninterruptible Power Supply (UPS) combines three functions in one system:

- It contains a transfer switch that chooses the active power input
- It contains some form of energy storage to bridge the time between the utility failure and the availability of the generators’ power
- It conditions the incoming power feed, removing voltage spikes or sags or harmonic distortions in the AC feed

## Cooling systems

Because of the great amount of heat generated by IT equipment in a data centre, its cooling system is usually one of the most expensive components. It is composed of coolers, heat exchangers and cold water tanks.

There are two main types of cooling systems: open-loop and closed-loop. We will now see both in detail.

### Open-loop cooling systems

Open-loop refers to the use of cold air outside of the data centre to either help the production of chilled water or directly cool servers. This is the most energy-efficient method of cooling, since it requires a lot less money compared to active chillers.

![](images/Pasted%20image%2020240606111455.png)

Open-loop cooling systems are the simplest type of cooling systems. The simplest topology we can find here is *fresh air cooling* (or *air economisation*), which is essentially letting outside air simply flow between servers in a data centre.

### Closed-loop cooling systems

The goal of a closed-loop system is to isolate and remove heat from the servers and transport it to a heat exchanger.

Closed-loop systems come in many forms, the most common being the air circuit on the data centre floor. This is an example of a two-loop cooling system.

The airflow through the underfloor plenum, the racks and back to the Computer Room Air Conditioning—CRAC for short—defines the primary air circuit: the **first loop**. The **second loop** instead—the liquid supply inside the CRAC units—leads directly from the CRAC to the external heat exchangers that discharge the heat to the environment.

![](images/Pasted%20image%2020240606112419.png)

Often, for larger data centres, a three-loop system like the one below is used:

![](images/Pasted%20image%2020240606112502.png)

While chillers can be thought of as water-cooled air conditioners, cooling towers cool a water stream by evaporating a portion of it into the atmosphere. The latter do not work very well in cold climates, since they require additional mechanisms to prevent ice formation.

### A critical comparison

Each topology we have described above presents tradeoffs in complexity, efficiency and cost:

- **Fresh air cooling** can be very efficient but doesn’t work in all climates, it requires filtering of airborne particulates and can introduce complex control problems
- **Two-loop systems** are easy to implement, relatively inexpensive to construct and offer isolation from external contamination, but typically have lower operational efficiency
- **Three-loop systems** are the most expensive to construct and have complex controls, but offer contaminant protection and energy efficiency

### In-rack, in-row and liquid cooling

At the server level, there are several techniques to move heat away from racks or server components themselves:

- **In-rack coolers** add an air-to-water heat exchanger at the back of a rack, so that the hot air exiting the servers immediately flows over water-cooled coils, reducing the path between server exhaust and CRAC input
- **In-row coolers** work just like in-rack ones, but the cooling coils are places adjacent to the racks instead of within it

Furthermore, to cool down the most power-hungry components—which are also typically the ones with highest power dissipation—we can use liquid cooling on the components themselves: it is sufficient to add liquid-cooled heat sinks to the components with some tubing which circulates cold fluid from the components to a liquid-to-air or liquid-to-liquid heat exchanger.

#### Container-based data centres

An evolution of in-row cooling comes from container-based data centres: server racks are placed inside a container which is then integrated with power distribution and heat exchangers.

![](images/Pasted%20image%2020240606114049.png)

---

## Data centre power consumption

The power consumption of data centres is an issue, since it can reach several MW. Furthermore, cooling requires about half the energy required by the IT equipment. Energy transformation also creates a large amount of energy waste.

Some interesting statistics about data centres:

- Data centres consume 3% of the global electricity supply, reaching about 416.2 TWh of energy
- Data centres produce 2% of total greenhouse gas emissions
- Data centres produce as much $CO_2$ as the Netherlands or Argentina

The table below sums up the costs of building and running a data centre nowadays:

| Amortised cost | Component      | Sub-components                    |
| -------------- | -------------- | --------------------------------- |
| ~45%           | Servers        | CPUs, memory, disks               |
| ~25%           | Infrastructure | UPSs, cooling, power distribution |
| ~15%           | Power draw     | Electrical utility costs          |
| ~15%           | Network        | Switches, links                   | 

In order to quantify the efficiency of a data centre, the **Power Usage Effectiveness** metric (PUE) was born: it is defined as the ratio of the total amount of energy used by a data centre to the energy delivered to the computing equipment:

$$\text{PUE} = {\text{Total facility power}\over\text{IT equipment power}}$$

Total facility power covers:

- IT systems
- Cooling
- UPSs
- Switching gear
- Generators
- Lights

The inverse of the PUE is known as **Data Centre Infrastructure Efficiency** (DCIE):

$$\text{DCIE} = \text{PUE}^{-1}$$

An average level of efficiency is usually correlated to a PUE of around 2, while an efficient data centre will reach a PUE of at least 1.5.

---

## Data centre tiers

Data centres can be classified into four tiers according to their infrastructure and availability:

| Tier | Requirements                                                                                                                                                                                                                                                                                                       |
| ---- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 1    | Single, non-redundant distribution path serving the IT equipment; non-redundant capacity components; basic site infrastructure with expected availability of 99.671%                                                                                                                                               |
| 2    | Meets or exceeds all tier 1 requirements; redundant site infrastructure capacity components with expected availability of 99.741%                                                                                                                                                                                  |
| 3    | Meets or exceeds all tier 2 requirements; multiple independent distribution paths serving the IT equipment; all IT equipment must be dual-powered and fully compatible with the topology of a site’s architecture; concurrently maintainable site infrastructure with expected availability of 99.982%             |
| 4    | Meets or exceeds all tier 3 requirements; all cooling equipment is independently dual-powered, including chillers and heating, ventilating and air-conditioning (HVAC) systems; fault-tolerant site infrastructure with electrical power storage and distribution facilities with expected availability of 99.995% | 
